{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b422767-457d-41d9-95f1-52f4d00864b9",
   "metadata": {},
   "source": [
    "Задание 1. Сравнить качество методов векторизации CountVectorizer и TF-IDF на примере задачи классификации текстов. Сделать выводы.\n",
    "\n",
    "Датасет:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "Темы: alt.atheism, misc.forsale, soc.religion.christian, talk.politics.mideast\n",
    "\n",
    "Алгоритм МО: логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0945cf-8ed5-4226-b423-6e87f17a2e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qtr/anaconda3/envs/rapids-23.12/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Классификация с использованием CountVectorizer:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.60      0.83      0.70       319\n",
      "          misc.forsale       1.00      0.42      0.59       390\n",
      "soc.religion.christian       0.73      0.94      0.83       398\n",
      " talk.politics.mideast       0.84      0.82      0.83       376\n",
      "\n",
      "              accuracy                           0.75      1483\n",
      "             macro avg       0.79      0.75      0.74      1483\n",
      "          weighted avg       0.80      0.75      0.74      1483\n",
      "\n",
      "Классификация с использованием TF-IDF:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.94      0.79      0.86       319\n",
      "          misc.forsale       0.90      0.99      0.94       390\n",
      "soc.religion.christian       0.86      0.95      0.91       398\n",
      " talk.politics.mideast       0.98      0.90      0.94       376\n",
      "\n",
      "              accuracy                           0.92      1483\n",
      "             macro avg       0.92      0.91      0.91      1483\n",
      "          weighted avg       0.92      0.92      0.91      1483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "categories = ['alt.atheism', 'misc.forsale', 'soc.religion.christian', 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "pipeline_count = make_pipeline(count_vectorizer, model)\n",
    "pipeline_tfidf = make_pipeline(tfidf_vectorizer, model)\n",
    "\n",
    "pipeline_count.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "pipeline_tfidf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "\n",
    "predictions_count = pipeline_count.predict(newsgroups_test.data)\n",
    "predictions_tfidf = pipeline_tfidf.predict(newsgroups_test.data)\n",
    "\n",
    "report_count = classification_report(newsgroups_test.target, predictions_count, target_names=newsgroups_test.target_names)\n",
    "report_tfidf = classification_report(newsgroups_test.target, predictions_tfidf, target_names=newsgroups_test.target_names)\n",
    "print(\"Классификация с использованием CountVectorizer:\\n\", report_count)\n",
    "print(\"Классификация с использованием TF-IDF:\\n\", report_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f7c06-10df-4a94-b2e2-305c39cf93d0",
   "metadata": {},
   "source": [
    "# Вывод\n",
    "\n",
    "tf-idf показывает около идеальное качество классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e3dc2-c63c-415e-9b00-e42c8ca8e6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "843c2866-deb3-4f36-aeef-fcfdadd9b96f",
   "metadata": {},
   "source": [
    "Задание 2. На примере задачи классификации текстов определить насколько предобработка текста (стемминг, лемматизация, стоп-слова и т.д.) влияет на качество обучения модели. Сделать выводы.  \n",
    "\n",
    "Датасет:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "Темы: alt.atheism, misc.forsale, soc.religion.christian, talk.politics.mideast\n",
    "\n",
    "Алгоритм МО: логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f01ce3-b04c-427a-bc06-8bdbf611bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qtr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/qtr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчет о классификации с предобработкой:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.90      0.82      0.86       319\n",
      "          misc.forsale       0.94      0.99      0.97       390\n",
      "soc.religion.christian       0.88      0.96      0.92       398\n",
      " talk.politics.mideast       0.98      0.90      0.94       376\n",
      "\n",
      "              accuracy                           0.92      1483\n",
      "             macro avg       0.93      0.92      0.92      1483\n",
      "          weighted avg       0.93      0.92      0.92      1483\n",
      "\n",
      "Отчет о классификации без предобработки:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.89      0.84      0.87       319\n",
      "          misc.forsale       0.95      0.99      0.97       390\n",
      "soc.religion.christian       0.89      0.97      0.93       398\n",
      " talk.politics.mideast       0.97      0.88      0.92       376\n",
      "\n",
      "              accuracy                           0.93      1483\n",
      "             macro avg       0.93      0.92      0.92      1483\n",
      "          weighted avg       0.93      0.93      0.93      1483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text, use_stemming=True, use_stopwords=True):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    if use_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "categories = ['alt.atheism', 'misc.forsale', 'soc.religion.christian', 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "texts_preprocessed = [preprocess_text(text) for text in newsgroups_train.data]\n",
    "\n",
    "vectorizer_preprocessed = CountVectorizer()\n",
    "X_train_preprocessed = vectorizer_preprocessed.fit_transform(texts_preprocessed)\n",
    "y_train = newsgroups_train.target\n",
    "\n",
    "model_preprocessed = LogisticRegression(max_iter=5000)\n",
    "model_preprocessed.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "vectorizer_raw = CountVectorizer()\n",
    "X_train_raw = vectorizer_raw.fit_transform(newsgroups_train.data)\n",
    "\n",
    "model_raw = LogisticRegression(max_iter=5000)\n",
    "model_raw.fit(X_train_raw, y_train)\n",
    "\n",
    "X_test_preprocessed = vectorizer_preprocessed.transform([preprocess_text(text) for text in newsgroups_test.data])\n",
    "X_test_raw = vectorizer_raw.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "report_preprocessed = classification_report(y_test, model_preprocessed.predict(X_test_preprocessed), target_names=newsgroups_test.target_names)\n",
    "report_raw = classification_report(y_test, model_raw.predict(X_test_raw), target_names=newsgroups_test.target_names)\n",
    "\n",
    "print(\"Отчет о классификации с предобработкой:\\n\", report_preprocessed)\n",
    "print(\"Отчет о классификации без предобработки:\\n\", report_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06474eae-0ea7-42b5-83f7-cddbc6b6d864",
   "metadata": {},
   "source": [
    "# Вывод\n",
    "\n",
    "конечно, с текст предобработкой лучше, но для нашего случая особо сильной разницы нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e9278-5e3f-466c-961d-40b263593adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8097e3-43eb-4b9f-b619-c93d03eab04c",
   "metadata": {},
   "source": [
    "Задание 3. Сравнить качество обучения классических методов машинного обучения и методов глубокого обучения на примере задачи классификации текстов. Сделать выводы.\n",
    "\n",
    "Датасет:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "Темы: alt.atheism, misc.forsale, soc.religion.christian, talk.politics.mideast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1c74af-1728-40c5-93b3-74088be68343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7868f9e-be4d-4816-9979-e493eb727a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'misc.forsale', 'soc.religion.christian', 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a03214c-3008-402c-9bd0-0263726ddee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be24f88a-b6aa-45ff-ad36-e109c4803cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наивный Байесовский классификатор:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90       319\n",
      "           1       0.98      0.98      0.98       390\n",
      "           2       0.91      0.97      0.94       398\n",
      "           3       0.98      0.92      0.95       376\n",
      "\n",
      "    accuracy                           0.95      1483\n",
      "   macro avg       0.95      0.94      0.94      1483\n",
      "weighted avg       0.95      0.95      0.95      1483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qtr/anaconda3/envs/rapids-23.12/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       319\n",
      "           1       0.95      0.99      0.97       390\n",
      "           2       0.88      0.97      0.92       398\n",
      "           3       0.97      0.88      0.92       376\n",
      "\n",
      "    accuracy                           0.92      1483\n",
      "   macro avg       0.92      0.92      0.92      1483\n",
      "weighted avg       0.92      0.92      0.92      1483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qtr/anaconda3/envs/rapids-23.12/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/qtr/anaconda3/envs/rapids-23.12/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/qtr/anaconda3/envs/rapids-23.12/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Линейная регрессия:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -8       0.00      0.00      0.00         0\n",
      "          -4       0.00      0.00      0.00         0\n",
      "          -2       0.00      0.00      0.00         0\n",
      "          -1       0.00      0.00      0.00         0\n",
      "           0       0.80      0.38      0.51       319\n",
      "           1       0.56      0.81      0.66       390\n",
      "           2       0.55      0.69      0.61       398\n",
      "           3       0.70      0.34      0.46       376\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.57      1483\n",
      "   macro avg       0.19      0.16      0.16      1483\n",
      "weighted avg       0.65      0.57      0.57      1483\n",
      "\n",
      "SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.60      0.69       319\n",
      "           1       0.71      0.96      0.81       390\n",
      "           2       0.77      0.80      0.78       398\n",
      "           3       0.88      0.72      0.79       376\n",
      "\n",
      "    accuracy                           0.78      1483\n",
      "   macro avg       0.79      0.77      0.77      1483\n",
      "weighted avg       0.79      0.78      0.77      1483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "print(\"Наивный Байесовский классификатор:\\n\", classification_report(y_test, nb_pred))\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "lr_pred = lr_classifier.predict(X_test)\n",
    "print(\"Логистическая регрессия:\\n\", classification_report(y_test, lr_pred))\n",
    "\n",
    "linreg_classifier = LinearRegression()\n",
    "linreg_classifier.fit(X_train, y_train)\n",
    "linreg_pred = np.rint(linreg_classifier.predict(X_test)).astype(int)\n",
    "print(\"Линейная регрессия:\\n\", classification_report(y_test, linreg_pred))\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "print(\"SVM:\\n\", classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95dafd-5183-4115-ac61-3904cf7c40d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2beacae-63b0-4b0a-b711-1ea82a63f996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da211240-fe9d-43c2-b6b5-9cf616ed7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'misc.forsale', 'soc.religion.christian', 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c784b939-8ffe-4f18-b1a3-e8fb6bd8bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() \n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "X_test = vectorizer.transform(newsgroups_test.data).toarray()\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train).float()\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count())\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9b1e23-c4dd-4e5a-9d47-1dc49e86ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        if out.dim() >= 2:\n",
    "            out = out.squeeze().unsqueeze(1).squeeze(1)\n",
    "        #out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        if out.dim() >= 2:\n",
    "            out = out.squeeze().unsqueeze(1).squeeze(1)\n",
    "        #out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class BiRNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiRNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        if out.dim() >= 2:\n",
    "            out = out.squeeze().unsqueeze(1).squeeze(1)\n",
    "        #out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        if out.dim() >= 2:\n",
    "            out = out.squeeze().unsqueeze(1).squeeze(1)\n",
    "        #out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ba3b1a-394a-4ce3-817f-5be723b0b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(6): \n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(texts)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Точность модели на тестовых данных: {accuracy}%')\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(all_labels, all_predictions))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9a0e25-40c6-4950-b5ad-dc6a885449c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение и оценка RNN модели\n",
      "Точность модели на тестовых данных: 93.39177343223196%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.89       319\n",
      "           1       0.97      0.99      0.98       390\n",
      "           2       0.87      0.98      0.92       398\n",
      "           3       0.97      0.91      0.94       376\n",
      "\n",
      "    accuracy                           0.93      1483\n",
      "   macro avg       0.94      0.93      0.93      1483\n",
      "weighted avg       0.94      0.93      0.93      1483\n",
      "\n",
      "RNN Точность: 93.39177343223196%\n",
      "\n",
      "Обучение и оценка LSTM модели\n",
      "Точность модели на тестовых данных: 93.66149696561025%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       319\n",
      "           1       0.96      0.99      0.98       390\n",
      "           2       0.91      0.97      0.94       398\n",
      "           3       0.97      0.90      0.93       376\n",
      "\n",
      "    accuracy                           0.94      1483\n",
      "   macro avg       0.94      0.93      0.93      1483\n",
      "weighted avg       0.94      0.94      0.94      1483\n",
      "\n",
      "LSTM Точность: 93.66149696561025%\n",
      "\n",
      "Обучение и оценка BiRNN модели\n",
      "Точность модели на тестовых данных: 93.5266351989211%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       319\n",
      "           1       0.98      0.99      0.98       390\n",
      "           2       0.88      0.96      0.92       398\n",
      "           3       0.97      0.90      0.94       376\n",
      "\n",
      "    accuracy                           0.94      1483\n",
      "   macro avg       0.94      0.93      0.93      1483\n",
      "weighted avg       0.94      0.94      0.94      1483\n",
      "\n",
      "BiRNN Точность: 93.5266351989211%\n",
      "\n",
      "Обучение и оценка BiLSTM модели\n",
      "Точность модели на тестовых данных: 94.80782198246797%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.92       319\n",
      "           1       0.97      0.99      0.98       390\n",
      "           2       0.91      0.97      0.94       398\n",
      "           3       0.98      0.93      0.95       376\n",
      "\n",
      "    accuracy                           0.95      1483\n",
      "   macro avg       0.95      0.95      0.95      1483\n",
      "weighted avg       0.95      0.95      0.95      1483\n",
      "\n",
      "BiLSTM Точность: 94.80782198246797%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  \n",
    "hidden_dim = 128\n",
    "output_dim = len(set(y_train))  \n",
    "\n",
    "models = {\n",
    "    \"RNN\": RNNModel(input_dim, hidden_dim, output_dim).to(device),\n",
    "    \"LSTM\": LSTMModel(input_dim, hidden_dim, output_dim).to(device),\n",
    "    \"BiRNN\": BiRNNModel(input_dim, hidden_dim, output_dim).to(device),\n",
    "    \"BiLSTM\": BiLSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Обучение и оценка {model_name} модели\")\n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, device)\n",
    "    print(f\"{model_name} Точность: {accuracy}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc545538-ff77-4c7d-9ce6-bcb6f037348a",
   "metadata": {},
   "source": [
    "# Вывод\n",
    "\n",
    "методы глубокого обучения показали себя значительно лучше, также использованние двухнаправленных rnn и lstm \n",
    "\n",
    "дало чуть лучшие реузльтаты по сравнению с классическими rnn и lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89414c-de8f-449b-8958-baf1f3b8e5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eed66264-7e59-449e-8a07-b6b359ce5a55",
   "metadata": {},
   "source": [
    "Задание 4. Используя модель Word2vec \n",
    "\n",
    "постройте эмбеддинги и визуализируйте их. Сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52ca6c-e986-42f5-b8f6-a84c7d66e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text, language='russian')\n",
    "\n",
    "with open('dostoevsky75.txt', 'r', encoding='utf-8') as file:\n",
    "    dostoevsky_text = preprocess_text(file.read(10000))\n",
    "\n",
    "with open('nietzsche.txt', 'r', encoding='utf-8') as file:\n",
    "    nietzsche_text = preprocess_text(file.read(10000))\n",
    "\n",
    "dostoevsky_tokens = tokenize(dostoevsky_text)\n",
    "nietzsche_tokens = tokenize(nietzsche_text)\n",
    "\n",
    "texts = [dostoevsky_tokens, nietzsche_tokens]\n",
    "\n",
    "model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=os.cpu_count())\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.key_to_index:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(np.array(tokens))\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(24, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6f650-cf28-4f2e-8906-f474ef233a8a",
   "metadata": {},
   "source": [
    "# вывод\n",
    "\n",
    "русское слово \"ясно\" и его формы,  а также англоязычное слово \"up\" встречались чаще всего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088cbd53-9353-48fe-8e46-6e8938867238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "add8f87b-01cf-4a85-8a57-efb18358791a",
   "metadata": {},
   "source": [
    "Задание 5. Сравнить две модели трансформеров на примере машинного перевода. \n",
    "\n",
    "Перевод следующий: русский -> английский -> испанский -> арабский -> русский. Сделать выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724766b-2361-483b-a495-99a9f7a84c58",
   "metadata": {},
   "source": [
    "#удачи скачать это столько моделей(я про хельсинки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba6cdb90-8b7f-49d0-807d-3a6904577277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный текст: Допустим, я тут что-то должен написать оригинальное. Я люблю хинкали и не только\n",
      "Перевод через Helsinki-NLP: Скажем так, я должен написать что-то оригинальное.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "def translate_helsinki(text, model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated][0]\n",
    "\n",
    "text = \"Допустим, я тут что-то должен написать оригинальное. Я люблю хинкали и не только\"\n",
    "\n",
    "translated_helsinki = translate_helsinki(text, \"Helsinki-NLP/opus-mt-ru-en\")\n",
    "translated_helsinki = translate_helsinki(translated_helsinki, \"Helsinki-NLP/opus-mt-en-es\")\n",
    "translated_helsinki = translate_helsinki(translated_helsinki, \"Helsinki-NLP/opus-mt-es-ar\")\n",
    "final_helsinki = translate_helsinki(translated_helsinki, \"Helsinki-NLP/opus-mt-ar-ru\")\n",
    "\n",
    "print(\"Оригинальный текст:\", text)\n",
    "print(\"Перевод через Helsinki-NLP:\", final_helsinki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "571712ce-5b54-4f97-8a54-31e87cf7917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный текст: Допустим, я тут что-то должен написать оригинальное. Я люблю хинкали и не только\n",
      "Перевод с использованием Facebook: Мы признаем, что мне нужно что-то написать здесь.Мне нравятся углы, а не только\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import torch\n",
    "\n",
    "def translate(text, model_name, src_lang, tgt_lang, device):\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "text = \"Допустим, я тут что-то должен написать оригинальное. Я люблю хинкали и не только\"\n",
    "model_name_facebook = \"facebook/m2m100_418M\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "translated_facebook = translate(text, model_name_facebook, \"ru\", \"en\", device)\n",
    "translated_facebook = translate(translated_facebook, model_name_facebook, \"en\", \"es\", device)\n",
    "translated_facebook = translate(translated_facebook, model_name_facebook, \"es\", \"ar\", device)\n",
    "translated_facebook = translate(translated_facebook, model_name_facebook, \"ar\", \"ru\", device)\n",
    "\n",
    "print(\"Оригинальный текст:\", text)\n",
    "print(\"Перевод с использованием Facebook:\", translated_facebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da564ebf-303f-45b9-b85a-da6874070614",
   "metadata": {},
   "source": [
    "Вывод: обе модели справились не очень, но шизофазия от фейсбука мне понравилась больше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0cfc1-df95-496b-b802-7712f75ccb85",
   "metadata": {},
   "source": [
    "Задание 6. Используя трансформеры сделать генерацию русскоязычного текста. \n",
    "\n",
    "Дообучить трансформер текстами Достоевского и повторить генерацию. Сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeebff08-f53d-4729-b561-72c8f5ec31eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 23:33:48.175238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-31 23:33:48.175278: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-31 23:33:48.175316: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-31 23:33:48.182314: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите строку:  Я люблю хинкали и кавказкую кухню\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я люблю хинкали и кавказкую кухню.\n",
      "\n",
      "\n",
      "21358947\tsozero\t2019-12-03 01:37:00\tПора уходить из страны, у которой уже нет денег \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\").to(device)\n",
    "\n",
    "text_to_generate = input(\"Введите строку: \")\n",
    "\n",
    "input_ids = tokenizer.encode(text_to_generate, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(input_ids, max_length=512, num_return_sequences=1, temperature=0.7, do_sample=True)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b51bfb-76d7-48ca-8016-d6a696828e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "model.to(device)\n",
    "\n",
    "file_path = \"dostoevskysmall.txt\"\n",
    "file_path2 =file_path[:70000]\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=file_path2,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=500,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f1b2f-8b86-4a3b-a510-0cb52e1fbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = \"./fine_tuned_gpt2\"  \n",
    "\n",
    "import os\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "print(\"Дообученная модель ru gpt3 small on GPT-2 сохранена в\", output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d66d8-95eb-48d9-9570-2cfb82c9c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "\n",
    "text_to_generate = input(\"Введите строку: \")\n",
    "\n",
    "input_ids = tokenizer.encode(text_to_generate, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(input_ids, max_length=512, num_return_sequences=1, temperature=0.7, do_sample=True)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
