{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fjnENCWzLJlY"},"outputs":[],"source":["import pandas as pd\n","import re\n","import torch\n","from nltk.tokenize import word_tokenize\n","from torchtext.vocab import build_vocab_from_iterator\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchmetrics as M\n","from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"markdown","metadata":{"id":"eipLiAxnLJld"},"source":["Создайте датасет на основе файла IMDB Dataset.csv (лежит в 03_embeddings/data). Разделите набор данных на обучающее и тестовое множество. Проведите предобработку данных. Минимальная предобработка данных: удаление html-тегов и знаков препинания. Разбейте документы на слова, закодируйте слова индексами, приведите каждый документ к одинаковому количеству токенов и преобразуйте в тензоры.\n","\n","Реализовав нейронную сеть при помощи библиотеки PyTorch, решите задачу классификации текстов. Отобразите confusion matrix и classification report, рассчитанные на основе обучающего и тестового множества.\n","\n","Вариант 1.\n","\n","Решите задачу задачу классификации, используя слой nn.Embedding. Для получения эмбеддингов документов выполните следующие действия:\n","1. Объедините эмбеддинги слов в документе в один длинный вектор размерности (n_tokens x embedding_dim)\n","2. Пропустите этот вектор через полносвязный слой, понизив размерность снова до embedding_dim\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GJOWY2fLJlg","outputId":"3325b9ca-56a5-49ac-ca3f-3444f34062f2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[one, of, the, other, reviewers, has, mentione...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[a, wonderful, little, production, the, filmin...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[basically, theres, a, family, where, a, littl...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  [one, of, the, other, reviewers, has, mentione...  positive\n","1  [a, wonderful, little, production, the, filmin...  positive\n","2  [i, thought, this, was, a, wonderful, way, to,...  positive\n","3  [basically, theres, a, family, where, a, littl...  negative\n","4  [petter, matteis, love, in, the, time, of, mon...  positive"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"IMDB Dataset.csv\")\n","\n","tags = re.compile(r'<.*?>')\n","punct = re.compile(r'[^\\w\\s]')\n","\n","def preprocess(text):\n","    text = re.sub(tags, '', text.lower())\n","    text = re.sub(punct, '', text)\n","    tokens = word_tokenize(text)\n","    return tokens\n","\n","data[\"review\"] = [preprocess(review) for review in data[\"review\"]]\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pYJQM4cwLJli"},"outputs":[],"source":["data_train, data_test = train_test_split(data, test_size=0.25, random_state=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOcSpNCXLJli"},"outputs":[],"source":["vocab = build_vocab_from_iterator(data_train[\"review\"], specials=['<pad>', '<unk>'])\n","vocab.set_default_index(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0djMec8LJli"},"outputs":[],"source":["def text_to_indices(doc):\n","    return [vocab[word] for word in doc]\n","\n","data_train['emb'] = data_train['review'].apply(lambda x: text_to_indices(x))\n","data_test['emb'] = data_test['review'].apply(lambda x: text_to_indices(x))\n","\n","max_length = max(data_train['emb'].apply(len).max(), data_test['emb'].apply(len).max())\n","\n","data_train['emb'] = data_train['emb'].apply(lambda x: x + [vocab['<pad>']] * (max_length - len(x)))\n","data_test['emb'] = data_test['emb'].apply(lambda x: x + [vocab['<pad>']] * (max_length - len(x)))\n","\n","data_train['sentiment'] = data_train['sentiment'].map({'positive': 1, 'negative': 0})\n","data_test['sentiment'] = data_test['sentiment'].map({'positive': 1, 'negative': 0})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rl94tfcJLJlj","outputId":"de15b7ea-c13f-49bb-8742-c93e7e68f784"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","      <th>emb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17552</th>\n","      <td>[lovely, music, beautiful, photography, some, ...</td>\n","      <td>0</td>\n","      <td>[1282, 206, 305, 1291, 46, 5, 131, 23, 2950, 4...</td>\n","    </tr>\n","    <tr>\n","      <th>20467</th>\n","      <td>[raising, victor, vargas, is, one, of, those, ...</td>\n","      <td>0</td>\n","      <td>[5404, 2615, 9735, 7, 28, 5, 141, 703, 235, 93...</td>\n","    </tr>\n","    <tr>\n","      <th>49715</th>\n","      <td>[my, abiding, love, of, italian, actress, luci...</td>\n","      <td>1</td>\n","      <td>[59, 19401, 110, 5, 1064, 522, 48661, 49834, 3...</td>\n","    </tr>\n","    <tr>\n","      <th>31896</th>\n","      <td>[so, the, wwe, has, done, it, they, have, pour...</td>\n","      <td>1</td>\n","      <td>[38, 2, 5616, 43, 224, 9, 34, 25, 12397, 129, ...</td>\n","    </tr>\n","    <tr>\n","      <th>11953</th>\n","      <td>[this, movie, might, not, put, the, catholic, ...</td>\n","      <td>1</td>\n","      <td>[10, 17, 222, 21, 263, 2, 3278, 1596, 8, 2, 11...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  review  sentiment  \\\n","17552  [lovely, music, beautiful, photography, some, ...          0   \n","20467  [raising, victor, vargas, is, one, of, those, ...          0   \n","49715  [my, abiding, love, of, italian, actress, luci...          1   \n","31896  [so, the, wwe, has, done, it, they, have, pour...          1   \n","11953  [this, movie, might, not, put, the, catholic, ...          1   \n","\n","                                                     emb  \n","17552  [1282, 206, 305, 1291, 46, 5, 131, 23, 2950, 4...  \n","20467  [5404, 2615, 9735, 7, 28, 5, 141, 703, 235, 93...  \n","49715  [59, 19401, 110, 5, 1064, 522, 48661, 49834, 3...  \n","31896  [38, 2, 5616, 43, 224, 9, 34, 25, 12397, 129, ...  \n","11953  [10, 17, 222, 21, 263, 2, 3278, 1596, 8, 2, 11...  "]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["data_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hbksfc3ULJlk"},"outputs":[],"source":["class Net(nn.Module):\n","  def __init__(self, num_embeddings):\n","    super().__init__()\n","    self.emb = nn.Embedding(\n","        num_embeddings=num_embeddings,\n","        embedding_dim=100,\n","    )\n","    self.fc = nn.Linear(in_features=100*max_length, out_features=2)\n","\n","  def forward(self, X):\n","\n","    e = self.emb(X)\n","    e = e.flatten(1)\n","    out = self.fc(e)\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7HBOMgSLJlk","outputId":"55802382-2471-4071-b555-3675792bb724"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch=0 train_acc.item()=0.5261066555976868 test_acc.item()=0.539680004119873\n","epoch=1 train_acc.item()=0.6425066590309143 test_acc.item()=0.7072799801826477\n","epoch=2 train_acc.item()=0.7543466687202454 test_acc.item()=0.7293599843978882\n","epoch=3 train_acc.item()=0.8054133057594299 test_acc.item()=0.675599992275238\n","epoch=4 train_acc.item()=0.8527466654777527 test_acc.item()=0.8022400140762329\n"]}],"source":["X_train = torch.LongTensor(data_train['emb'].tolist())\n","y_train = torch.LongTensor(data_train['sentiment'].tolist())\n","\n","X_test = torch.LongTensor(data_test['emb'].tolist())\n","y_test = torch.LongTensor(data_test['sentiment'].tolist())\n","\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=100)\n","test_loader = DataLoader(test_dataset, batch_size=100)\n","\n","n_epochs = 5\n","lr = 0.001\n","\n","model = Net(num_embeddings=len(vocab))\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","for epoch in range(n_epochs):\n","\n","  model.train()\n","  acc_m = M.Accuracy(task=\"binary\")\n","  for X, y in train_loader:\n","    y_pred = model(X)\n","    loss = criterion(y_pred, y)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    acc_m.update(y_pred.argmax(dim=1), y)\n","\n","  model.eval()\n","  test_acc_m = M.Accuracy(task=\"binary\")\n","  with torch.no_grad():\n","      for X, y in test_loader:\n","          y_test_pred = model(X)\n","          test_acc_m.update(y_test_pred.argmax(dim=1), y)\n","\n","  train_acc = acc_m.compute()\n","  test_acc = test_acc_m.compute()\n","  print(f\"{epoch=} {train_acc.item()=} {test_acc.item()=}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQPBQ6KmLJll"},"outputs":[],"source":["model.eval()\n","test_predictions = []\n","test_labels = []\n","train_predictions = []\n","train_labels = []\n","\n","with torch.no_grad():\n","\n","    for X_train_batch, y_train_batch in train_loader:\n","        y_train_pred = model(X_train_batch)\n","        train_predictions.extend(y_train_pred.argmax(dim=1).tolist())\n","        train_labels.extend(y_train_batch.tolist())\n","\n","    for X_test_batch, y_test_batch in test_loader:\n","        y_test_pred = model(X_test_batch)\n","        test_predictions.extend(y_test_pred.argmax(dim=1).tolist())\n","        test_labels.extend(y_test_batch.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQwAvio5LJlm","outputId":"f2bae310-0b3f-487e-a6eb-84ef1c0b743b"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.96     18735\n","           1       0.97      0.96      0.96     18765\n","\n","    accuracy                           0.96     37500\n","   macro avg       0.96      0.96      0.96     37500\n","weighted avg       0.96      0.96      0.96     37500\n","\n"]}],"source":["# train\n","class_report = classification_report(train_labels, train_predictions)\n","print(class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00T69LAZLJlm","outputId":"f5106922-66a8-497a-e873-7fdc543fd6ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.84      0.81      6265\n","           1       0.83      0.76      0.79      6235\n","\n","    accuracy                           0.80     12500\n","   macro avg       0.80      0.80      0.80     12500\n","weighted avg       0.80      0.80      0.80     12500\n","\n"]}],"source":["# test\n","class_report = classification_report(test_labels, test_predictions)\n","print(class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSIjMrq4LJlm","outputId":"5a8e92ac-4580-491e-b809-872b02e4e5da"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[18175   560]\n"," [  771 17994]]\n"]}],"source":["# train\n","conf_matrix = confusion_matrix(train_labels, train_predictions)\n","print(conf_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVxrhmHFLJln","outputId":"07f70024-91ef-4a21-90b9-7f80a6a58285"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[5276  989]\n"," [1483 4752]]\n"]}],"source":["# test\n","conf_matrix = confusion_matrix(test_labels, test_predictions)\n","print(conf_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snOBBKARLJln"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}